# –ü—É—Ç—å: backend/rssfeed/management/commands/import_rss.py
# –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–º–ø–æ—Ä—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–æ–∫, —Å—Å—ã–ª–∫–∞, –¥–∞—Ç–∞, –∫–∞—Ä—Ç–∏–Ω–∫–∞, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ç–µ–∫—Å—Ç/summary).
# –û–±–Ω–æ–≤–ª–µ–Ω–∏—è:
#   ‚Ä¢ ‚úÖ –õ–µ–Ω—Ç—É –∫–∞—á–∞–µ–º —á–µ—Ä–µ–∑ rssfeed.net.get_rss_bytes() (—Ä–µ—Ç—Ä–∞–∏, –±—ç–∫–æ—Ñ—Ñ, –ø–µ—Ä-–¥–æ–º–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã; –¥–ª—è aif.ru read‚âà28‚Äì38s).
#   ‚Ä¢ ‚úÖ feedparser.parse() –ø–æ–ª—É—á–∞–µ—Ç bytes, –∞ –Ω–µ URL ‚Üí –Ω–∏–∫–∞–∫–∏—Ö —Ç–∞–π–º–∞—É—Ç–æ–≤ –Ω–∞ 10s –æ—Ç —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –≤—ã–∑–æ–≤–æ–≤.
#   ‚Ä¢ ‚úÖ fetch_page() —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è rssfeed.net.fetch_url(), –∑–∞—Ç–µ–º (—Ñ–æ–ª–±—ç–∫) requests.get(..., timeout=8).
#   ‚Ä¢ ‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç --allow-empty: –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–∂–µ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–µ—Ç–∫–æ–π ‚Äú[–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞]‚Äù.
#   ‚Ä¢ ‚úÖ –ü–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è cleanup_broken_news().
#   ‚Ä¢ –í—Å—è –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã. –ù–ò–ß–ï–ì–û –õ–ò–®–ù–ï–ì–û –ù–ï –£–î–ê–õ–ï–ù–û.

import re
import time
import html
from datetime import datetime
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser
import requests
from bs4 import BeautifulSoup

from django.core.management.base import BaseCommand
from django.utils import timezone
from django.utils.text import slugify
from django.db import transaction

from news.models import ImportedNews, Category, NewsSource
from news.utils.cleanup import cleanup_broken_news

# üîå –ù–∞—à –Ω–∞–¥—ë–∂–Ω—ã–π —Å–µ—Ç–µ–≤–æ–π —Å–ª–æ–π
from rssfeed.net import get_rss_bytes, fetch_url

# --- –ü–ê–†–ê–ú–ï–¢–†–´ –ö–ê–ß–ï–°–¢–í–ê -------------------------------------------------------

REQUEST_TIMEOUT = 8     # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è HTML-—Ñ–æ–ª–±—ç–∫–∞ (–ù–ï –¥–ª—è RSS!)
MIN_SUMMARY_CHARS = 120
MIN_PARAGRAPHS = 1
MAX_SUMMARY_CHARS = 1200
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

TRACKING_PARAMS = {
    "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
    "yclid", "fbclid", "gclid", "utm_referrer", "utm_name"
}

TRASH_PATTERNS = [
    r"–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ.*?$",
    r"–ü–æ–¥—Ä–æ–±–Ω–µ–µ.*?$",
    r"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏.*?$",
    r"–ò—Å—Ç–æ—á–Ω–∏–∫:\s*.+$",
]
TRASH_RE = re.compile("|".join(TRASH_PATTERNS), re.IGNORECASE | re.MULTILINE)

# --- –£–¢–ò–õ–ò–¢–´ ------------------------------------------------------------------

def strip_tracking_params(url: str) -> str:
    try:
        parsed = urlparse(url)
        if not parsed.query:
            return url
        qs = dict(parse_qsl(parsed.query, keep_blank_values=True))
        qs = {k: v for k, v in qs.items() if k not in TRACKING_PARAMS}
        new_query = urlencode(qs, doseq=True)
        return urlunparse(parsed._replace(query=new_query))
    except Exception:
        return url

def html_to_text_preserve_paragraphs(raw_html: str) -> str:
    if not raw_html:
        return ""
    soup = BeautifulSoup(raw_html, "lxml")
    for br in soup.find_all(["br"]):
        br.replace_with("\n")
    for p in soup.find_all(["p", "div", "li"]):
        if p.text:
            p.insert_after(soup.new_string("\n"))
    text = soup.get_text(separator="\n")
    text = html.unescape(text)
    text = re.sub(r"\r\n", "\n", text)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    text = TRASH_RE.sub("", text).strip()
    return text

def first_paragraphs(text: str, max_chars: int = MAX_SUMMARY_CHARS) -> str:
    if not text:
        return ""
    parts = [p.strip() for p in text.split("\n") if p.strip()]
    if not parts:
        return ""
    picked, total = [], 0
    for p in parts:
        if total and total + len(p) + 2 > max_chars:
            break
        picked.append(p)
        total += len(p) + 2
        if len(picked) >= 3:
            break
    return "\n\n".join(picked).strip()[:max_chars].rstrip()

def extract_link(entry) -> str:
    for key in ("link", "id", "href"):
        url = getattr(entry, key, None) or entry.get(key)
        if url:
            return strip_tracking_params(url)
    return ""

def extract_title(entry) -> str:
    val = entry.get("title")
    if val:
        return html.unescape(BeautifulSoup(val, "lxml").get_text(" ").strip())
    return ""

def extract_raw_html_from_entry(entry) -> str:
    if entry.get("content"):
        try:
            return entry["content"][0].get("value") or ""
        except Exception:
            pass
    if entry.get("summary_detail") and entry["summary_detail"].get("value"):
        return entry["summary_detail"]["value"]
    if entry.get("summary"):
        return entry["summary"]
    if entry.get("description"):
        return entry["description"]
    return ""

def extract_image_from_entry(entry) -> str:
    media_content = entry.get("media_content") or entry.get("media:content")
    if media_content:
        if isinstance(media_content, list) and media_content:
            return media_content[0].get("url") or media_content[0].get("@url")
        if isinstance(media_content, dict):
            return media_content.get("url") or media_content.get("@url")
    if entry.get("enclosures"):
        for enc in entry["enclosures"]:
            url = enc.get("url")
            if url and (enc.get("type", "").startswith("image") or url.lower().endswith((".jpg", ".jpeg", ".png", ".webp", ".gif"))):
                return url
    thumb = entry.get("media_thumbnail") or entry.get("media:thumbnail")
    if thumb:
        if isinstance(thumb, list) and thumb:
            return thumb[0].get("url") or thumb[0].get("@url")
        if isinstance(thumb, dict):
            return thumb.get("url") or thumb.get("@url")
    return ""

def extract_published(entry) -> datetime | None:
    for key in ("published_parsed", "updated_parsed"):
        if entry.get(key):
            try:
                ts = time.mktime(entry[key])
                return timezone.make_aware(datetime.fromtimestamp(ts), timezone=timezone.utc)
            except Exception:
                pass
    return None

def extract_category(entry) -> str:
    tags = entry.get("tags") or []
    if tags:
        tag0 = tags[0]
        label = tag0.get("label") or tag0.get("term")
        if label:
            return label.strip()
    return "–õ–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"

def fetch_page(url: str) -> BeautifulSoup | None:
    """
    1) –ü—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ –Ω–∞—à —É—Å—Ç–æ–π—á–∏–≤—ã–π fetch_url() (–ø–µ—Ä-–¥–æ–º–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã).
    2) –§–æ–ª–±—ç–∫: –±—ã—Å—Ç—Ä—ã–π requests.get(..., timeout=8).
    """
    try:
        res = fetch_url(url)
        if res.status == 200 and res.data:
            return BeautifulSoup(res.data, "lxml")
    except Exception:
        pass
    try:
        resp = requests.get(
            url,
            timeout=REQUEST_TIMEOUT,
            headers={
                "User-Agent": USER_AGENT,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            },
        )
        if resp.status_code != 200 or not resp.text:
            return None
        return BeautifulSoup(resp.text, "lxml")
    except Exception:
        return None

def page_extract_text(soup: BeautifulSoup) -> str:
    if not soup:
        return ""
    md = soup.find("meta", attrs={"name": "description"})
    if md and md.get("content"):
        return md["content"].strip()
    ogd = soup.find("meta", attrs={"property": "og:description"})
    if ogd and ogd.get("content"):
        return ogd["content"].strip()
    candidates = []
    for attr, val in (("id", "article"), ("id", "content"), ("class_", "article"), ("class_", "content"), ("class_", "post")):
        try:
            if attr == "id":
                node = soup.find(attrs={"id": re.compile(val, re.I)})
            else:
                node = soup.find(attrs={"class": re.compile(val, re.I)})
            if node:
                candidates.append(node)
        except Exception:
            pass
    if not candidates:
        candidates = soup.find_all("article") or [soup.body]
    for node in candidates:
        if not node:
            continue
        ps = [p.get_text(" ", strip=True) for p in node.find_all("p")]
        ps = [p for p in ps if p and len(p) > 40]
        if ps:
            return "\n\n".join(ps[:3]).strip()
    return ""

def page_extract_image(soup: BeautifulSoup) -> str:
    if not soup:
        return ""
    og = soup.find("meta", attrs={"property": "og:image"})
    if og and og.get("content"):
        return og["content"].strip()
    tw = soup.find("meta", attrs={"name": "twitter:image"})
    if tw and tw.get("content"):
        return tw["content"].strip()
    art = soup.find("article")
    img = art.find("img") if art else soup.find("img")
    if img and img.get("src"):
        return img["src"].strip()
    return ""

def model_has_field(model, field_name: str) -> bool:
    return any(getattr(f, "name", "") == field_name for f in model._meta.get_fields())

def assign_if_exists(instance, **kwargs):
    for k, v in kwargs.items():
        if model_has_field(instance.__class__, k):
            setattr(instance, k, v)

# --- –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ----------------------------------------------------------

class Command(BaseCommand):
    help = "–ò–º–ø–æ—Ä—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Å –æ—á–∏—Å—Ç–∫–æ–π —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ–ª–±—ç–∫–æ–º –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."

    def add_arguments(self, parser):
        parser.add_argument(
            "--only",
            nargs="*",
            help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∏–º–ø–æ—Ä—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ slug (–Ω–∞–ø—Ä–∏–º–µ—Ä: ria-novosti tass lenta-ru).",
        )
        parser.add_argument(
            "--allow-empty",
            action="store_true",
            help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å –¥–∞–∂–µ –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–æ–¥—Å—Ç–∞–≤–ª—è—è '[–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞]').",
        )

    @transaction.atomic
    def handle(self, *args, **options):
        only_slugs = set(options.get("only") or [])
        allow_empty = options.get("allow_empty", False)

        sources = list(NewsSource.objects.filter(is_active=True).order_by("name"))
        if only_slugs:
            sources = [s for s in sources if s.slug in only_slugs]

        if not sources:
            self.stdout.write(self.style.WARNING("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ NewsSource."))
            return

        total_new, total_skipped = 0, 0

        for src in sources:
            if not src.feed_url:
                self.stdout.write(self.style.WARNING(f"‚úñ –ü—Ä–æ–ø—É—â–µ–Ω '{src.name}': –Ω–µ—Ç feed_url"))
                continue

            self.stdout.write(self.style.NOTICE(f"‚Üí –ò–º–ø–æ—Ä—Ç –∏–∑ {src.name} ({src.feed_url})"))

            # –í–ê–ñ–ù–û: –∫–∞—á–∞–µ–º –ª–µ–Ω—Ç—É —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –Ω–∞—à fetcher (–ø–µ—Ä-–¥–æ–º–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã, —Ä–µ—Ç—Ä–∞–∏)
            try:
                data, enc, meta = get_rss_bytes(src.feed_url)
                feed = feedparser.parse(data)
            except Exception as e:
                self.stdout.write(self.style.ERROR(f"  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏/–ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–µ–Ω—Ç—ã: {e}"))
                continue

            if not feed or not feed.get("entries"):
                self.stdout.write(self.style.WARNING("  –í –ª–µ–Ω—Ç–µ –Ω–µ—Ç –∑–∞–ø–∏—Å–µ–π."))
                continue

            added, skipped = 0, 0

            for entry in feed["entries"]:
                try:
                    link = extract_link(entry)
                    if not link:
                        skipped += 1
                        continue

                    title = extract_title(entry)
                    raw_html = extract_raw_html_from_entry(entry)
                    img_from_feed = extract_image_from_entry(entry)
                    published_dt = extract_published(entry) or timezone.now()

                    text = html_to_text_preserve_paragraphs(raw_html)
                    text = first_paragraphs(text, MAX_SUMMARY_CHARS)

                    if (len(text) < MIN_SUMMARY_CHARS) or (len([p for p in text.split("\n") if p.strip()]) < MIN_PARAGRAPHS):
                        soup = fetch_page(link)
                        fb_txt = page_extract_text(soup)
                        fb_txt = html_to_text_preserve_paragraphs(fb_txt)
                        fb_txt = first_paragraphs(fb_txt, MAX_SUMMARY_CHARS)
                        if len(fb_txt) >= len(text):
                            text = fb_txt
                        if not img_from_feed:
                            img_from_page = page_extract_image(soup)
                            if img_from_page:
                                img_from_feed = img_from_page

                    para_count = len([p for p in (text or "").split("\n") if p.strip()])
                    if not text or len(text) < MIN_SUMMARY_CHARS or para_count < MIN_PARAGRAPHS:
                        if not allow_empty:
                            skipped += 1
                            self.stdout.write(self.style.WARNING(
                                f"  ‚Äî –ü—Ä–æ–ø—É—Å–∫: ¬´{title[:60]}‚Ä¶¬ª (–º–∞–ª—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤., {para_count} –∞–±–∑.)"
                            ))
                            continue
                        else:
                            text = "[–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞]"

                    cat_name = extract_category(entry).strip() or "–õ–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
                    cat_slug = slugify(cat_name) or "lenta-novostei"
                    category, _ = Category.objects.get_or_create(slug=cat_slug, defaults={"name": cat_name})

                    existing = ImportedNews.objects.filter(link=link).first()
                    if existing:
                        assign_if_exists(existing, summary=text)
                        if img_from_feed:
                            if model_has_field(ImportedNews, "image"):
                                assign_if_exists(existing, image=img_from_feed)
                            elif model_has_field(ImportedNews, "image_url"):
                                assign_if_exists(existing, image_url=img_from_feed)
                            elif model_has_field(ImportedNews, "cover_image"):
                                assign_if_exists(existing, cover_image=img_from_feed)
                        assign_if_exists(existing, category=category, source=src, published_at=published_dt)
                        existing.save(update_fields=[f.name for f in existing._meta.fields if f.name not in ("id",)])
                        continue

                    news = ImportedNews()
                    assign_if_exists(news, title=title)
                    assign_if_exists(news, summary=text)
                    assign_if_exists(news, link=link)
                    assign_if_exists(news, category=category)
                    assign_if_exists(news, source=src)
                    assign_if_exists(news, published_at=published_dt)
                    if img_from_feed:
                        if model_has_field(ImportedNews, "image"):
                            assign_if_exists(news, image=img_from_feed)
                        elif model_has_field(ImportedNews, "image_url"):
                            assign_if_exists(news, image_url=img_from_feed)
                        elif model_has_field(ImportedNews, "cover_image"):
                            assign_if_exists(news, cover_image=img_from_feed)

                    news.save()
                    added += 1

                except Exception as e:
                    skipped += 1
                    self.stdout.write(self.style.ERROR(f"  –û—à–∏–±–∫–∞ –ø–æ –∑–∞–ø–∏—Å–∏: {e}"))

            total_new += added
            total_skipped += skipped
            self.stdout.write(self.style.SUCCESS(f"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ: {added}  |  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}"))

        self.stdout.write(self.style.SUCCESS(f"–ì–û–¢–û–í–û. –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_new}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {total_skipped}"))
        cleanup_broken_news(self.stdout)
